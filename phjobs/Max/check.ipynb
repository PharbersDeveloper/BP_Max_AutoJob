{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phcli Jupyter Template\n",
    "# \n",
    "# 使用手册：\n",
    "# 1. 请将全局变量定义在第一个输入区内\n",
    "# 2. Phcli 会自动在第二个输入区初始化 Spark Session\n",
    "# 3. 所有 print 会在 phcli maxauto dag 后自动转为 logger.debug() 方法\n",
    "# 4. 请在第三个输入区开始编码，phcli maxauto dag 后会全部归类为一个方法\n",
    "\n",
    "\n",
    "# Config defined in here\n",
    "\n",
    "############## == config == ###################\n",
    "job_name = \"check\"\n",
    "job_runtime = \"python3\"\n",
    "job_command = \"submit\"\n",
    "job_timeout = 720.0\n",
    "############## == config == ###################\n",
    "\n",
    "\n",
    "# Variables defined in here\n",
    "\n",
    "############## == input args == ###################\n",
    "max_path = \"s3a://ph-max-auto/v0.0.1-2020-06-08/\"\n",
    "project_name = \"Empty\"\n",
    "time_left = \"Empty\"\n",
    "time_right = \"Empty\"\n",
    "out_dir = \"Empty\"\n",
    "if_others = \"False\"\n",
    "############## == input args == ###################\n",
    "\n",
    "############## == output args == ###################\n",
    "c = 'abc'\n",
    "d = 'def'\n",
    "############## == output args == ###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Spark Session\n",
    "import os\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "\n",
    "# ENV\n",
    "os.environ[\"HADOOP_HOME\"] = \"/usr/local/hadoop\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/usr/local/spark\"\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = \"AKIAWPBDTVEAEU44ZAGT\"\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"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\"\n",
    "os.environ[\"AWS_DEFAULT_REGION\"] = \"cn-northwest-1\"\n",
    "\n",
    "# prepare\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"yarn\") \\\n",
    "    .appName(\"Jupyter Keep-Alive Session\") \\\n",
    "    .config(\"spark.driver.memory\", \"1g\") \\\n",
    "    .config(\"spark.executor.cores\", \"1\") \\\n",
    "    .config(\"spark.executor.instance\", \"1\") \\\n",
    "    .config(\"spark.executor.memory\", \"1g\") \\\n",
    "    .config('spark.sql.codegen.wholeStage', False) \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "access_key = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "secret_key = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "if access_key:\n",
    "    spark._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", access_key)\n",
    "    spark._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", secret_key)\n",
    "    spark._jsc.hadoopConfiguration().set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    spark._jsc.hadoopConfiguration().set(\"com.amazonaws.services.s3.enableV4\", \"true\")\n",
    "    spark._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"s3.cn-northwest-1.amazonaws.com.cn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from phcli.ph_logs.ph_logs import phs3logger\n",
    "# from pyspark.sql.types import *\n",
    "from pyspark.sql.types import StringType, IntegerType, DoubleType\n",
    "from pyspark.sql import functions as func\n",
    "import os\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType, udf, col\n",
    "from pyspark.sql import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "max_path = \"s3a://ph-max-auto/v0.0.1-2020-06-08/\"\n",
    "project_name = \"Eisai\"\n",
    "time_left = \"201701\"\n",
    "time_right = \"202009\"\n",
    "out_dir = \"202009\"\n",
    "'''\n",
    "max_path = kwargs[\"max_path\"]\n",
    "project_name = kwargs[\"project_name\"]\n",
    "time_left = kwargs[\"time_left\"]\n",
    "time_right = kwargs[\"time_right\"]\n",
    "out_dir = kwargs[\"out_dir\"]\n",
    "if_others = kwargs[\"if_others\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输入文件\n",
    "product_map_path = max_path + \"/\" + project_name + \"/\" + out_dir + \"/prod_mapping\"\n",
    "time_range = str(time_left) + '_' + str(time_right)\n",
    "if if_others == 'True':\n",
    "    max_result_city_path = max_path + \"/\" + project_name + \"/\" + out_dir + \"/others_box/MAX_result/MAX_result_\" + time_range + \"_city_level\"\n",
    "    panel_path = max_path + \"/\" + project_name + \"/\" + out_dir + \"/others_box/panel_result_box\"\n",
    "else:\n",
    "    max_result_city_path = max_path + \"/\" + project_name + \"/\" + out_dir + \"/MAX_result/MAX_result_\" + time_range + \"_city_level\"\n",
    "    panel_path = max_path + \"/\" + project_name + \"/\" + out_dir + \"/panel_result\"\n",
    "\n",
    "hospital_map_path = max_path + \"/Common_files/CPA_GYC_hospital_map\"\n",
    "ims_city_map_path = max_path + '/Common_files/IMS_mapping_citycode.csv'\n",
    "IMS_flat_files_dir = max_path + \"/Common_files/IMS_flat_files/\" + out_dir\n",
    "\n",
    "ims_Sales_path = IMS_flat_files_dir + '/cn_IMS_Sales_Fdata_' + out_dir + '_1.csv'\n",
    "ims_mol_lkp_path = IMS_flat_files_dir + '/cn_mol_lkp_' + out_dir + '_1.csv'\n",
    "ims_mol_ref_path = IMS_flat_files_dir + '/cn_mol_ref_' + out_dir + '_1.csv'\n",
    "\n",
    "# 输出文件\n",
    "if if_others == 'True':\n",
    "    check_path = max_path + '/' + project_name + '/' + out_dir + '/others_box/max_check/'\n",
    "else:\n",
    "    check_path = max_path + '/' + project_name + '/' + out_dir + '/max_check/'\n",
    "check_1_path = check_path + '/check_1_byProduct.csv'\n",
    "check_2_path = check_path + '/check_2_byCity.csv'\n",
    "check_3_path = check_path + '/check_3_补数比例.csv'\n",
    "check_4_path = check_path + '/check_4_放大比例.csv'\n",
    "check_5_path = check_path + '/check_5_产品个数.csv'\n",
    "check_6_path = check_path + '/check_6_样本医院个数.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========  数据执行  ============\n",
    "\n",
    "# ====  一. 函数定义  ====\n",
    "\n",
    "def deal_ID_length(df):\n",
    "    # ID不足7位的补足0到6位\n",
    "    # 国药诚信医院编码长度是7位数字，cpa医院编码是6位数字。\n",
    "    df = df.withColumn(\"ID\", df[\"ID\"].cast(StringType()))\n",
    "    # 去掉末尾的.0\n",
    "    df = df.withColumn(\"ID\", func.regexp_replace(\"ID\", \"\\\\.0\", \"\"))\n",
    "    df = df.withColumn(\"ID\", func.when(func.length(df.ID) < 7, func.lpad(df.ID, 6, \"0\")).otherwise(df.ID))\n",
    "    return df\n",
    "\n",
    "@udf(StringType())\n",
    "def city_change(name):\n",
    "    # 城市名统一\n",
    "    if name in [\"福州市\", \"厦门市\", \"泉州市\"]:\n",
    "        newname = \"福厦泉\"\n",
    "    elif name in [\"苏州市\", \"无锡市\"]:\n",
    "        newname = \"苏锡城市群\"\n",
    "    else:\n",
    "        newname = name\n",
    "    return newname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====  二. 数据准备  ====  \n",
    "\n",
    "# 1. prod_map 文件\n",
    "product_map = spark.read.parquet(product_map_path)\n",
    "# a. 列名清洗统一\n",
    "if project_name == \"Sanofi\" or project_name == \"AZ\":\n",
    "    product_map = product_map.withColumnRenamed(product_map.columns[21], \"pfc\")\n",
    "if project_name == \"Eisai\":\n",
    "    product_map = product_map.withColumnRenamed(product_map.columns[22], \"pfc\")\n",
    "for i in product_map.columns:\n",
    "    if i in [\"标准通用名\", \"通用名_标准\", \"药品名称_标准\", \"S_Molecule_Name\"]:\n",
    "        product_map = product_map.withColumnRenamed(i, \"通用名\")\n",
    "    if i in [\"packcode\", \"Pack_ID\", \"Pack_Id\", \"PackID\", \"packid\"]:\n",
    "        product_map = product_map.withColumnRenamed(i, \"pfc\")\n",
    "    if i in [\"min1_标准\"]:\n",
    "        product_map = product_map.withColumnRenamed(i, \"min2\")\n",
    "# b. 选取需要的列\n",
    "product_map = product_map \\\n",
    "                .select(\"min2\", \"pfc\", \"通用名\") \\\n",
    "                .withColumn(\"pfc\", product_map[\"pfc\"].cast(IntegerType())) \\\n",
    "                .distinct()\n",
    "# c. pfc为0统一替换为null\n",
    "product_map = product_map.withColumn(\"pfc\", func.when(product_map.pfc == 0, None).otherwise(product_map.pfc)).distinct()\n",
    "# d. min2处理\n",
    "product_map = product_map.withColumnRenamed(\"pfc\", \"Pack_ID\") \\\n",
    "                .withColumn(\"min2\", func.regexp_replace(\"min2\", \"&amp;\", \"&\")) \\\n",
    "                .withColumn(\"min2\", func.regexp_replace(\"min2\", \"&lt;\", \"<\")) \\\n",
    "                .withColumn(\"min2\", func.regexp_replace(\"min2\", \"&gt;\", \">\")) \\\n",
    "                .withColumnRenamed(\"通用名\", \"标准通用名\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. hospital_map 文件\n",
    "hospital_map = spark.read.parquet(hospital_map_path)\n",
    "hospital_map = hospital_map.select(\"医院编码\", \"医院名称\", \"等级\" ,\"标准化省份\", \"标准化城市\").distinct() \\\n",
    "                            .withColumnRenamed('医院编码', 'ID')\n",
    "hospital_map = deal_ID_length(hospital_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. ims 文件\n",
    "ims_Sales = spark.read.csv(ims_Sales_path, header=True)\n",
    "ims_Sales = ims_Sales.withColumn('Period_Code', func.regexp_replace(\"Period_Code\", \"M\", \"\")).distinct()\n",
    "# 匹配英文通用名\n",
    "ims_mol_lkp = spark.read.csv(ims_mol_lkp_path, header=True).distinct()\n",
    "ims_mol_ref = spark.read.csv(ims_mol_ref_path, header=True).distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 合并复方分子：将Pack_ID相同的Molecule_Desc合并到一起\n",
    "ims_mol = ims_mol_lkp.join(ims_mol_ref, ims_mol_lkp['Molecule_ID']==ims_mol_ref['Molecule_Id'], how='left')\n",
    "\n",
    "Source_window = Window.partitionBy(\"Pack_ID\").orderBy(func.col('Molecule_Desc'))\n",
    "rank_window = Window.partitionBy(\"Pack_ID\").orderBy(func.col('Molecule_Desc').desc())\n",
    "\n",
    "ims_mol = ims_mol.select(\"Pack_ID\", \"Molecule_Desc\").distinct() \\\n",
    "                .select(\"Pack_ID\", func.collect_list(func.col('Molecule_Desc')).over(Source_window).alias('Molecule_Composition'), \n",
    "                                                    func.rank().over(rank_window).alias('rank')).persist()\n",
    "ims_mol = ims_mol.where(ims_mol.rank == 1).drop('rank')\n",
    "ims_mol = ims_mol.withColumn('Molecule_Composition', func.concat_ws('_', func.col('Molecule_Composition'))) \\\n",
    "                    .orderBy('Pack_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ims城市范围\n",
    "ims_city_map = spark.read.csv(ims_city_map_path, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. max 文件\n",
    "max_data = spark.read.parquet(max_result_city_path)\n",
    "max_data_m = max_data.join(product_map, max_data.Prod_Name==product_map.min2, how='left') \\\n",
    "                    .withColumn('Date', col('Date').cast(IntegerType())) \\\n",
    "                    .withColumn('PANEL', col('PANEL').cast(IntegerType())) \\\n",
    "                    .withColumn(\"Pack_ID\", func.when(col('Pack_ID').isNull(), func.lit(0)).otherwise(col('Pack_ID'))).distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. panel 文件\n",
    "panel = spark.read.parquet(panel_path)\n",
    "panel = panel.withColumn('Date', col('Date').cast(IntegerType()))\n",
    "panel = panel.where((col('Date') >= int(time_left)) & (col('Date') <= int(time_right)))\n",
    "panel = deal_ID_length(panel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1.产品层面：样本 vs MAX vs IMS 对比，group by Packid&Date\\n2.bycity比对：样本与ims的gap应小于10%\\n3.补数金额比例：总金额的 1-3%\\n4.放大比例范围：1-3倍\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ====  三. 检查步骤  ====\n",
    "'''\n",
    "1.产品层面：样本 vs MAX vs IMS 对比，group by Packid&Date\n",
    "2.bycity比对：样本与ims的gap应小于10%\n",
    "3.补数金额比例：总金额的 1-3%\n",
    "4.放大比例范围：1-3倍\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.产品层面：样本 vs MAX vs IMS 对比，group by Packid&Date\n",
    "# MAX\n",
    "check_max_1 = max_data_m.groupby('Date', 'Prod_Name', '标准通用名', 'Pack_ID') \\\n",
    "                        .agg(func.sum('Predict_Sales').alias('max_Sales'))\n",
    "\n",
    "# CPA\n",
    "check_cpa_1 = max_data_m.where(col('PANEL')==1) \\\n",
    "                        .groupby('Date', 'Prod_Name', '标准通用名', 'Pack_ID') \\\n",
    "                        .agg(func.sum('Predict_Sales').alias('cpa_Sales')) \n",
    "\n",
    "# IMS\n",
    "ims_Sales_1 = ims_Sales.where(col('Geography_id') == 'CHT') \\\n",
    "                        .groupby('Pack_ID', 'Period_Code').agg(func.sum('LC').alias('ims_Sales')) \\\n",
    "                        .withColumn('month_pack', func.concat_ws('_', col('Period_Code'), col('Pack_ID'))) \\\n",
    "                        .join(ims_mol, on='Pack_ID', how='left')\n",
    "# 匹配筛选出max月份有的pack\n",
    "check_max_1_tmp = check_max_1.withColumn('month_pack', func.concat_ws('_', col('Date'), col('Pack_ID'))) \\\n",
    "                            .withColumn('flag', func.lit(1)) \\\n",
    "                            .select('month_pack', 'flag').distinct()\n",
    "ims_Sales_2 = ims_Sales_1.join(check_max_1_tmp, on='month_pack', how='left')\n",
    "ims_Sales_2 = ims_Sales_2.where(~col('flag').isNull()) \\\n",
    "                        .groupby('Period_Code', 'Pack_ID', 'Molecule_Composition') \\\n",
    "                        .agg(func.sum('ims_Sales').alias('ims_Sales')) \\\n",
    "                        .withColumn('Period_Code', col('Period_Code').cast(IntegerType())) \\\n",
    "                        .withColumnRenamed('Period_Code', 'Date') \\\n",
    "                        .withColumn(\"Pack_ID\", col(\"Pack_ID\").cast(IntegerType()))\n",
    "\n",
    "check_result = check_max_1.join(check_cpa_1, on=['Date', 'Prod_Name', '标准通用名', 'Pack_ID'], how='full') \\\n",
    "                           .join(ims_Sales_2, on=['Date', 'Pack_ID'], how='full').persist()\n",
    "check_result = check_result.withColumn('Rank',func.row_number() \\\n",
    "                                           .over(Window.partitionBy('Date', 'Pack_ID').orderBy(col('Pack_ID').desc()))).persist()\n",
    "check_result = check_result.withColumn('ims_Sales', func.when(col('Rank') > 1, func.lit(0)).otherwise(col('ims_Sales'))) \\\n",
    "                            .withColumn('Year', func.substring(col('Date'), 0, 4)) \\\n",
    "                            .select('Date', 'Prod_Name', '标准通用名', 'Pack_ID', 'Molecule_Composition', 'max_Sales', 'cpa_Sales', \n",
    "                                    'ims_Sales', 'Rank', 'Year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_result = check_result.repartition(1)\n",
    "check_result.write.format(\"csv\").option(\"header\", \"true\") \\\n",
    "        .mode(\"overwrite\").save(check_1_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.按城市检查：样本 vs MAX vs IMS 对比，group by City&Date\n",
    "\n",
    "# MAX\n",
    "ims_city = ims_city_map.select('City').distinct().toPandas()['City'].tolist()\n",
    "max_data_m_all = max_data_m.withColumn('City', func.lit('全国')) \\\n",
    "                            .withColumn('Province', func.lit('全国'))\n",
    "\n",
    "check_max_bycity = max_data_m.withColumn('City', city_change(col('City'))) \\\n",
    "                            .union(max_data_m_all) \\\n",
    "                            .where(col('City').isin(ims_city)) \\\n",
    "                            .groupby('Date', 'Province', 'City', 'Prod_Name', '标准通用名', 'Pack_ID') \\\n",
    "                            .agg(func.sum('Predict_Sales').alias('max_Sales'))\n",
    "# CPA\n",
    "check_cpa_bycity = max_data_m.where(col('PANEL') == 1) \\\n",
    "                            .withColumn('City', city_change(col('City'))) \\\n",
    "                            .union(max_data_m_all.where(col('PANEL') == 1)) \\\n",
    "                            .where(col('City').isin(ims_city)) \\\n",
    "                            .groupby('Date', 'Province', 'City', 'Prod_Name', '标准通用名', 'Pack_ID') \\\n",
    "                            .agg(func.sum('Predict_Sales').alias('cpa_Sales'))\n",
    "# IMS\n",
    "ims_Sales_bycity = ims_Sales.join(ims_city_map.select(\"AUDIT SHORT DESC\", \"Province\", \"City\"), \n",
    "                                ims_Sales['Geography_id']==ims_city_map['AUDIT SHORT DESC'], how='left')\n",
    "ims_Sales_bycity = ims_Sales_bycity.where(~col('Province').isNull()) \\\n",
    "                                    .groupby('Pack_ID', 'Period_Code', 'Province', 'City') \\\n",
    "                                    .agg(func.sum('LC').alias('ims_Sales')) \\\n",
    "                                    .withColumn('month_pack', func.concat_ws('_', col('Period_Code'), col('Pack_ID'))) \\\n",
    "                                    .join(ims_mol, on='Pack_ID', how='left')\n",
    "# 匹配筛选出max月份有的pack\n",
    "check_max_bycity_tmp = check_max_bycity.withColumn('month_pack', func.concat_ws('_', col('Date'), col('Pack_ID'))) \\\n",
    "                                        .withColumn('flag', func.lit(1)) \\\n",
    "                                        .select('month_pack', 'flag').distinct()\n",
    "ims_Sales_bycity_2 = ims_Sales_bycity.join(check_max_bycity_tmp, on='month_pack', how='left') \\\n",
    "                                    .where(~col('flag').isNull()) \\\n",
    "                                    .groupby('Period_Code', 'Province', 'City', 'Pack_ID', 'Molecule_Composition') \\\n",
    "                                    .agg(func.sum('ims_Sales').alias('ims_Sales')) \\\n",
    "                                    .withColumn('Period_Code', col('Period_Code').cast(IntegerType())) \\\n",
    "                                    .withColumnRenamed('Period_Code', 'Date') \\\n",
    "                                    .withColumn(\"Pack_ID\", col(\"Pack_ID\").cast(IntegerType()))\n",
    "\n",
    "check_result_bycity = check_max_bycity.join(check_cpa_bycity, on=['Date', 'Province', 'City', 'Prod_Name', '标准通用名', 'Pack_ID'], how='full') \\\n",
    "                           .join(ims_Sales_bycity_2, on=['Date', 'Province', 'City', 'Pack_ID'], how='full').persist()\n",
    "check_result_bycity = check_result_bycity.withColumn('Rank',func.row_number() \\\n",
    "                                           .over(Window.partitionBy('Date', 'Province', 'City', 'Pack_ID').orderBy(col('Pack_ID').desc()))).persist()\n",
    "check_result_bycity = check_result_bycity.withColumn('ims_Sales', func.when(col('Rank') > 1, func.lit(0)).otherwise(col('ims_Sales'))) \\\n",
    "                            .withColumn('Year', func.substring(col('Date'), 0, 4)) \\\n",
    "                            .select('Date', 'Province', 'City', 'Prod_Name', '标准通用名', 'Pack_ID', 'max_Sales', 'cpa_Sales', 'ims_Sales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_result_bycity = check_result_bycity.repartition(1)\n",
    "check_result_bycity.write.format(\"csv\").option(\"header\", \"true\") \\\n",
    "    .mode(\"overwrite\").save(check_2_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.补数金额比例：占总金额 1-3%\n",
    "check_sales = panel.groupby('Date', 'Molecule', 'add_flag').agg(func.sum('Sales').alias('Sales'))\n",
    "check_sales = check_sales.groupBy('Date', 'Molecule').pivot('add_flag').agg(func.sum('Sales')).persist()\n",
    "check_sales = check_sales.withColumn('补数比例', col('1')/col('0')) \\\n",
    "                          .withColumnRenamed('Molecule', '标准通用名') \n",
    "\n",
    "check_sales = check_sales.repartition(1)\n",
    "check_sales.write.format(\"csv\").option(\"header\", \"true\") \\\n",
    "    .mode(\"overwrite\").save(check_3_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.放大比例\n",
    "check_max_sales = max_data_m.groupby('Date', '标准通用名', 'PANEL').agg(func.sum('Predict_Sales').alias('金额'))\n",
    "check_max_sales = check_max_sales.groupBy('Date', '标准通用名').pivot('PANEL').agg(func.sum('金额')).persist()\n",
    "check_max_sales = check_max_sales.withColumn('放大比例', (col('1')+col('0'))/col('1'))\n",
    "\n",
    "check_max_sales = check_max_sales.repartition(1)\n",
    "check_max_sales.write.format(\"csv\").option(\"header\", \"true\") \\\n",
    "    .mode(\"overwrite\").save(check_4_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.每个月的产品个数（min2）MAX，CPA\n",
    "check_max_1_count = check_max_1.select('Date', 'Prod_Name').distinct() \\\n",
    "                    .groupby('Date').count() \\\n",
    "                    .withColumnRenamed('count', 'max_prd')\n",
    "check_cpa_1_count = check_cpa_1.select('Date', 'Prod_Name').distinct() \\\n",
    "                    .groupby('Date').count() \\\n",
    "                    .withColumnRenamed('count', 'cpa_prd')\n",
    "data_prd = check_max_1_count.join(check_cpa_1_count, on='Date', how='left') \\\n",
    "                            .orderBy('Date')\n",
    "\n",
    "data_prd = data_prd.repartition(1)\n",
    "data_prd.write.format(\"csv\").option(\"header\", \"true\") \\\n",
    "    .mode(\"overwrite\").save(check_5_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.每个月的CPA医院个数\n",
    "data_cpa_pha = panel.join(hospital_map.select('ID', '医院名称').distinct(), on='ID', how='left') \\\n",
    "                    .select('Date', '医院名称').distinct() \\\n",
    "                    .groupby('Date').count() \\\n",
    "                    .withColumnRenamed('count', '医院个数') \\\n",
    "                    .orderBy('Date')\n",
    "data_cpa_pha = data_cpa_pha.repartition(1)\n",
    "data_cpa_pha.write.format(\"csv\").option(\"header\", \"true\") \\\n",
    "    .mode(\"overwrite\").save(check_6_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

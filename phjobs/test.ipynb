{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phcli Jupyter Template\n",
    "# \n",
    "# 使用手册：\n",
    "# 1. 请将全局变量定义在第一个输入区内\n",
    "# 2. Phcli 会自动在第二个输入区初始化 Spark Session\n",
    "# 3. 所有 print 会在 phcli maxauto dag 后自动转为 logger.debug() 方法\n",
    "# 4. 请在第三个输入区开始编码，phcli maxauto dag 后会全部归类为一个方法\n",
    "\n",
    "\n",
    "# Config defined in here\n",
    "\n",
    "############## == config == ###################\n",
    "job_name = \"test\"\n",
    "job_runtime = \"python3\"\n",
    "job_command = \"submit\"\n",
    "job_timeout = 10.0\n",
    "############## == config == ###################\n",
    "\n",
    "\n",
    "# Variables defined in here\n",
    "\n",
    "############## == input args == ###################\n",
    "a = 123\n",
    "b = 456\n",
    "############## == input args == ###################\n",
    "\n",
    "############## == output args == ###################\n",
    "c = 'abc'\n",
    "d = 'def'\n",
    "############## == output args == ###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Spark Session\n",
    "import os\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "\n",
    "# ENV\n",
    "os.environ[\"HADOOP_HOME\"] = \"/usr/local/hadoop\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/usr/local/spark\"\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = \"AKIAWPBDTVEAEU44ZAGT\"\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"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\"\n",
    "os.environ[\"AWS_DEFAULT_REGION\"] = \"cn-northwest-1\"\n",
    "\n",
    "# prepare\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"yarn\") \\\n",
    "    .appName(\"Jupyter Keep-Alive Session\") \\\n",
    "    .config(\"spark.driver.memory\", \"1g\") \\\n",
    "    .config(\"spark.executor.cores\", \"1\") \\\n",
    "    .config(\"spark.executor.instance\", \"1\") \\\n",
    "    .config(\"spark.executor.memory\", \"1g\") \\\n",
    "    .config('spark.sql.codegen.wholeStage', False) \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "access_key = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "secret_key = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "if access_key:\n",
    "    spark._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", access_key)\n",
    "    spark._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", secret_key)\n",
    "    spark._jsc.hadoopConfiguration().set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    spark._jsc.hadoopConfiguration().set(\"com.amazonaws.services.s3.enableV4\", \"true\")\n",
    "    spark._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"s3.cn-northwest-1.amazonaws.com.cn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phcli.ph_logs.ph_logs import phs3logger\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.types import StringType, IntegerType, DoubleType\n",
    "from pyspark.sql import functions as func\n",
    "import os\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType, udf, col\n",
    "import time\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numTrees=100, minInstancesPerNode=2, maxDepth=8\n",
    "import time\n",
    "a=time.asctime()\n",
    "model = rf.fit(data)\n",
    "b=time.asctime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'思则凯': {'福厦泉': '4', '北京市': '5'}, '雪诺同': {'福厦泉': '4'}}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "market_city_brand = '思则凯:福厦泉_4|北京市_5,雪诺同:福厦泉_4'\n",
    "\n",
    "market_city_brand_dict={}\n",
    "for each in market_city_brand.replace(\" \",\"\").split(\",\"):\n",
    "    market_name = each.split(\":\")[0]\n",
    "    if market_name not in market_city_brand_dict.keys():\n",
    "        market_city_brand_dict[market_name]={}\n",
    "    city_brand = each.split(\":\")[1]\n",
    "    for each in city_brand.replace(\" \",\"\").split(\"|\"): \n",
    "        city = each.split(\"_\")[0]\n",
    "        brand = each.split(\"_\")[1]\n",
    "        market_city_brand_dict[market_name][city]=brand\n",
    "market_city_brand_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.read.parquet('s3a://ph-max-auto/v0.0.1-2020-06-08/神州/202011/panel_result/')\n",
    "\n",
    "df = df.repartition(1)\n",
    "df.write.format(\"csv\").option(\"header\", \"true\") \\\n",
    "        .mode(\"overwrite\").save('s3a://ph-max-auto/v0.0.1-2020-06-08/神州/202011/panel_result.csv/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.read.csv('s3a://ph-max-auto/v0.0.1-2020-06-08/神州/202011/MAX_result/MAX_result_202001_202011_city_level.csv/',header=True)\n",
    "df = df.toPandas()\n",
    "df.to_csv('/home/ywyuan/tmp_file/MAX_result_202001_202011_city_level.csv',index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = 'Eisai'\n",
    "outdir = '202009'\n",
    "outdir_local = \"/home/ywyuan/tmp_file\"\n",
    "\n",
    "max_path = 's3a://ph-max-auto/v0.0.1-2020-06-08/'\n",
    "raw_data_check_path = max_path + '/' + project_name + '/' + outdir + '/max_check/'\n",
    "check_1_path = raw_data_check_path + '/check_1_byProduct.csv'\n",
    "check_2_path = raw_data_check_path + '/check_2_byCity.csv'\n",
    "check_3_path = raw_data_check_path + '/check_3_补数比例.csv'\n",
    "check_4_path = raw_data_check_path + '/check_4_放大比例.csv'\n",
    "check_5_path = raw_data_check_path + '/check_5_产品个数.csv'\n",
    "check_6_path = raw_data_check_path + '/check_6_样本医院个数.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>医院个数</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>201701</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>201702</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Date 医院个数\n",
       "0  201701    1\n",
       "1  201702    1"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_6 = spark.read.csv(check_6_path, header=True)\n",
    "check_6 = check_6.toPandas()\n",
    "check_6.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Prod_Name</th>\n",
       "      <th>标准通用名</th>\n",
       "      <th>Pack_ID</th>\n",
       "      <th>Molecule_Composition</th>\n",
       "      <th>max_Sales</th>\n",
       "      <th>cpa_Sales</th>\n",
       "      <th>ims_Sales</th>\n",
       "      <th>Rank</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>201701</td>\n",
       "      <td>天晴甘美-正大天晴|注射剂|10ML50MG|2|正大天晴药业集团股份有限公司</td>\n",
       "      <td>异甘草酸镁</td>\n",
       "      <td>3081604</td>\n",
       "      <td>ISOGLYCYRRHIZIC ACID</td>\n",
       "      <td>3988437.2466281895</td>\n",
       "      <td>3423691.5600000005</td>\n",
       "      <td>4717251.12</td>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>201701</td>\n",
       "      <td>扑米酮-上海信谊|片剂|0.25G|100|上海信谊黄河制药有限公司</td>\n",
       "      <td>扑米酮</td>\n",
       "      <td>3173902</td>\n",
       "      <td>PRIMIDONE</td>\n",
       "      <td>15760.327934619649</td>\n",
       "      <td>13999.0</td>\n",
       "      <td>5390.9</td>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Date                                Prod_Name  标准通用名  Pack_ID  \\\n",
       "0  201701  天晴甘美-正大天晴|注射剂|10ML50MG|2|正大天晴药业集团股份有限公司  异甘草酸镁  3081604   \n",
       "1  201701       扑米酮-上海信谊|片剂|0.25G|100|上海信谊黄河制药有限公司    扑米酮  3173902   \n",
       "\n",
       "   Molecule_Composition           max_Sales           cpa_Sales   ims_Sales  \\\n",
       "0  ISOGLYCYRRHIZIC ACID  3988437.2466281895  3423691.5600000005  4717251.12   \n",
       "1             PRIMIDONE  15760.327934619649             13999.0      5390.9   \n",
       "\n",
       "  Rank  Year  \n",
       "0    1  2017  \n",
       "1    1  2017  "
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_2.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3a://ph-max-auto/v0.0.1-2020-06-08//Eisai/202009/max_check//check_2_byCity.csv'"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_2_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.read.parquet('s3a://ph-max-auto/v0.0.1-2020-06-08/贝达/202011/panel_result/')\n",
    "df.repartition(1).write.format(\"csv\").option(\"header\", \"true\") \\\n",
    "    .mode(\"overwrite\").save(\"s3a://ph-max-auto/v0.0.1-2020-06-08/贝达/202011/panel_result.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Date: string, max_prd: string, cpa_prd: string]"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=spark.read.csv('s3a://ph-max-auto/v0.0.1-2020-06-08/Eisai/202009/max_check/check_5_产品个数.csv/', header=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phcli.ph_logs.ph_logs import phs3logger\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.types import StringType, IntegerType, DoubleType\n",
    "from pyspark.sql import functions as func\n",
    "import os\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType, udf, greatest, least\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输入\n",
    "max_path = 's3a://ph-max-auto/v0.0.1-2020-06-08/'\n",
    "project_name = '贝达'\n",
    "outdir = '202011'\n",
    "minimum_product_sep = '|'\n",
    "minimum_product_columns = \"Brand, Form, Specifications, Pack_Number, Manufacturer\"\n",
    "current_year = int(2020)\n",
    "current_month = int(11)\n",
    "three = int(3)\n",
    "twelve = int(12)\n",
    "test = 'False'\n",
    "if minimum_product_sep == \"kong\":\n",
    "    minimum_product_sep = \"\"\n",
    "minimum_product_columns = minimum_product_columns.replace(\" \",\"\").split(\",\")\n",
    "product_map_path = max_path + '/' + project_name + '/' + outdir + '/prod_mapping'\n",
    "cpa_pha_mapping_path = max_path + '/' + project_name + '/cpa_pha_mapping'\n",
    "if test == 'True':\n",
    "    raw_data_path = max_path + '/' + project_name + '/' + outdir + '/raw_data_check/raw_data'\n",
    "else:\n",
    "    raw_data_path = max_path + '/' + project_name + '/' + outdir + '/raw_data'\n",
    "raw_data_check_path = max_path + '/' + project_name + '/' + outdir + '/raw_data_check/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输出\n",
    "check_result_path = raw_data_check_path + '/check_result.csv'\n",
    "check_1_path = raw_data_check_path + '/check_1_每个月产品个数.csv'\n",
    "check_2_path = raw_data_check_path + '/check_2_各产品历史月份销量.csv'\n",
    "check_3_path = raw_data_check_path + '/check_3_历史医院个数.csv'\n",
    "check_5_path = raw_data_check_path + '/check_5_最近12期每家医院每个月的金额规模.csv'\n",
    "check_8_path = raw_data_check_path + '/check_8_每个医院每个月产品个数.csv'\n",
    "check_9_1_path = raw_data_check_path + '/check_9_1_所有产品每个月金额.csv'\n",
    "check_9_2_path = raw_data_check_path + '/check_9_2_所有产品每个月份额.csv'\n",
    "check_9_3_path = raw_data_check_path + '/check_9_3_所有产品每个月排名.csv'\n",
    "check_10_path = raw_data_check_path + '/check_10_在售产品医院个数.csv'\n",
    "check_11_path = raw_data_check_path + '/check_11_全部医院历史贡献率等级.csv'\n",
    "tmp_1_path  = raw_data_check_path + '/tmp_1'\n",
    "tmp_2_path  = raw_data_check_path + '/tmp_2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================\n",
    "\n",
    "MTH = current_year*100 + current_month\n",
    "PREMTH = MTH - 1\n",
    "\n",
    "# 当前月的前3个月\n",
    "if three > (current_month - 1):\n",
    "    diff = three - current_month\n",
    "    RQMTH = [i for i in range((current_year - 1)*100 +12 - diff , (current_year - 1)*100 + 12 + 1)] + [i for i in range(MTH - current_month + 1 , MTH)]\n",
    "else:\n",
    "    RQMTH = [i for i in range(MTH - current_month + 1 , MTH)][-three:]\n",
    "\n",
    "# 当前月的前12个月\n",
    "if twelve > (current_month - 1):\n",
    "    diff = twelve - current_month\n",
    "    mat_month = [i for i in range((current_year - 1)*100 + 12 - diff , (current_year - 1)*100 + 12 + 1)] + [i for i in range(MTH - current_month + 1 , MTH)]\n",
    "else:\n",
    "    mat_month = [i for i in range(MTH - current_month + 1 , MTH)][-twelve:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ================= 数据执行 ==================\t\n",
    "Raw_data = spark.read.parquet(raw_data_path)\n",
    "Raw_data = Raw_data.withColumn('Date', Raw_data['Date'].cast(IntegerType())) \\\n",
    "                .withColumn('Units', Raw_data['Units'].cast(DoubleType())) \\\n",
    "                .withColumn('Sales', Raw_data['Sales'].cast(DoubleType()))\n",
    "if 'Pack_ID' in Raw_data.columns:\n",
    "    Raw_data = Raw_data.drop('Pack_ID')\n",
    "\n",
    "# 生成min1\n",
    "if project_name != 'Mylan':\n",
    "    Raw_data = Raw_data.withColumn('Brand_bak', Raw_data.Brand)\n",
    "    Raw_data = Raw_data.withColumn('Brand', func.when((Raw_data.Brand.isNull()) | (Raw_data.Brand == 'NA'), Raw_data.Molecule).\n",
    "                                                otherwise(Raw_data.Brand))\n",
    "    Raw_data = Raw_data.withColumn(\"min1\", func.when(Raw_data[minimum_product_columns[0]].isNull(), func.lit(\"NA\")).\n",
    "                                       otherwise(Raw_data[minimum_product_columns[0]]))\n",
    "    for col in minimum_product_columns[1:]:\n",
    "        Raw_data = Raw_data.withColumn(col, Raw_data[col].cast(StringType()))\n",
    "        Raw_data = Raw_data.withColumn(\"min1\", func.concat(\n",
    "            Raw_data[\"min1\"],\n",
    "            func.lit(minimum_product_sep),\n",
    "            func.when(func.isnull(Raw_data[col]), func.lit(\"NA\")).otherwise(Raw_data[col])))\n",
    "    Raw_data = Raw_data.withColumn('Brand', Raw_data.Brand_bak).drop('Brand_bak')\n",
    "\n",
    "# 产品匹配表处理 \n",
    "product_map = spark.read.parquet(product_map_path)\n",
    "# a. 列名清洗统一\n",
    "if project_name == \"Sanofi\" or project_name == \"AZ\":\n",
    "    product_map = product_map.withColumnRenamed(product_map.columns[21], \"pfc\")\n",
    "if project_name == \"Eisai\":\n",
    "    product_map = product_map.withColumnRenamed(product_map.columns[22], \"pfc\")\n",
    "for col in product_map.columns:\n",
    "    if col in [\"标准通用名\", \"通用名_标准\", \"药品名称_标准\", \"S_Molecule_Name\"]:\n",
    "        product_map = product_map.withColumnRenamed(col, \"通用名\")\n",
    "    if col in [\"min1_标准\"]:\n",
    "        product_map = product_map.withColumnRenamed(col, \"min2\")\n",
    "    if col in [\"packcode\", \"Pack_ID\", \"Pack_Id\", \"PackID\", \"packid\"]:\n",
    "        product_map = product_map.withColumnRenamed(col, \"pfc\")\n",
    "    if col in [\"商品名_标准\", \"S_Product_Name\"]:\n",
    "        product_map = product_map.withColumnRenamed(col, \"标准商品名\")\n",
    "    if col in [\"剂型_标准\", \"Form_std\", \"S_Dosage\"]:\n",
    "        product_map = product_map.withColumnRenamed(col, \"标准剂型\")\n",
    "    if col in [\"规格_标准\", \"Specifications_std\", \"药品规格_标准\", \"S_Pack\"]:\n",
    "        product_map = product_map.withColumnRenamed(col, \"标准规格\")\n",
    "    if col in [\"包装数量2\", \"包装数量_标准\", \"Pack_Number_std\", \"S_PackNumber\", \"最小包装数量\"]:\n",
    "        product_map = product_map.withColumnRenamed(col, \"标准包装数量\")\n",
    "    if col in [\"标准企业\", \"生产企业_标准\", \"Manufacturer_std\", \"S_CORPORATION\", \"标准生产厂家\"]:\n",
    "        product_map = product_map.withColumnRenamed(col, \"标准生产企业\")\n",
    "if project_name == \"Janssen\" or project_name == \"NHWA\":\n",
    "    if \"标准剂型\" not in product_map.columns:\n",
    "        product_map = product_map.withColumnRenamed(\"剂型\", \"标准剂型\")\n",
    "    if \"标准规格\" not in product_map.columns:\n",
    "        product_map = product_map.withColumnRenamed(\"规格\", \"标准规格\")\n",
    "    if \"标准生产企业\" not in product_map.columns:\n",
    "        product_map = product_map.withColumnRenamed(\"生产企业\", \"标准生产企业\")\n",
    "    if \"标准包装数量\" not in product_map.columns:\n",
    "        product_map = product_map.withColumnRenamed(\"包装数量\", \"标准包装数量\")\n",
    "\n",
    "# b. 选取需要的列\n",
    "product_map = product_map \\\n",
    "                .select(\"min1\", \"min2\", \"pfc\", \"通用名\", \"标准商品名\", \"标准剂型\", \"标准规格\", \"标准包装数量\", \"标准生产企业\") \\\n",
    "                .distinct() \\\n",
    "                .withColumnRenamed(\"标准商品名\", \"商品名\") \\\n",
    "                .withColumnRenamed(\"标准剂型\", \"剂型\") \\\n",
    "                .withColumnRenamed(\"标准规格\", \"规格\") \\\n",
    "                .withColumnRenamed(\"标准包装数量\", \"包装数量\") \\\n",
    "                .withColumnRenamed(\"标准生产企业\", \"生产企业\") \\\n",
    "                .withColumnRenamed(\"pfc\", \"Pack_ID\")\n",
    "\n",
    "product_map = product_map.withColumn('Pack_ID', product_map.Pack_ID.cast(IntegerType()))\n",
    "\n",
    "# 匹配产品匹配表，标准化min2通用名商品名\n",
    "Raw_data_1 = Raw_data.join(product_map.select('min1','min2','通用名','商品名','Pack_ID').dropDuplicates(['min1']), on='min1', how='left')\n",
    "Raw_data_1 = Raw_data_1.groupby('ID', 'Date', 'min2', '通用名','商品名','Pack_ID') \\\n",
    "                        .agg(func.sum('Sales').alias('Sales'), func.sum('Units').alias('Units')) \\\n",
    "                        .withColumnRenamed('min2', 'Prod_Name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "#========== check_11 ==========\n",
    "\n",
    "# 全部医院历史贡献率等级\n",
    "allmonth = Raw_data.select('Date').distinct().sort('Date').toPandas()['Date'].values\n",
    "\n",
    "check_11 = Raw_data.select('ID').distinct().sort('ID')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|    ID|\n",
      "+------+\n",
      "|010011|\n",
      "+------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "check_11.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf=Raw_data.where(Raw_data.Date=='202001').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "month = pdf['Date'][0]\n",
    "pdf = pdf.groupby('ID')['Sales'].agg('sum').reset_index()\n",
    "pdf = pdf.sort_values('Sales', ascending=False)\n",
    "pdf['cumsum'] = pdf['Sales'].cumsum()\n",
    "pdf['sum'] = pdf['Sales'].sum()\n",
    "pdf['con_add'] = pdf['cumsum']/pdf['sum']\n",
    "pdf['level'] = np.where(pdf['con_add']*10 > 10, 10, np.ceil(pdf['con_add']*10))\n",
    "pdf ['month'] = str(month)\n",
    "pdf = pdf[['ID', 'level', 'month']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID        object\n",
       "level    float64\n",
       "month     object\n",
       "dtype: object"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_cumsum_rank(pdf, grouplist, sumcol):\n",
    "    month = pdf['Date'][0]\n",
    "    pdf = pdf.groupby(grouplist)[sumcol].agg('sum').reset_index()\n",
    "    pdf = pdf.sort_values(sumcol, ascending=False)\n",
    "    pdf['cumsum'] = pdf[sumcol].cumsum()\n",
    "    pdf['sum'] = pdf[sumcol].sum()\n",
    "    pdf['con_add'] = pdf['cumsum']/pdf['sum']\n",
    "    pdf['level'] = np.where(pdf['con_add']*10 > 10, 10, np.ceil(pdf['con_add']*10))\n",
    "    pdf ['month'] = str(month)\n",
    "    pdf = pdf[grouplist + ['level', 'month']]\n",
    "    pdf['level'].astype('int')\n",
    "    return pdf\n",
    "\n",
    "schema = StructType([\n",
    "        StructField(\"ID\", StringType(), True),\n",
    "        StructField(\"level\", IntegerType(), True),\n",
    "        StructField(\"month\", StringType(), True)\n",
    "        ])\n",
    "@pandas_udf(schema, PandasUDFType.GROUPED_MAP)\n",
    "def pudf_cumsum_rank(pdf):\n",
    "    return func_pandas_cumsum_level(pdf, grouplist=['ID'], sumcol='Sales')\n",
    "\n",
    "out = Raw_data.groupby([\"Date\"]).apply(pudf_cumsum_rank)\n",
    "out2 = out.groupby('ID').pivot('month').agg(func.sum('level')).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+------+\n",
      "|    ID|level| month|\n",
      "+------+-----+------+\n",
      "|450191|    1|202003|\n",
      "+------+-----+------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "out.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Date', 'ID', 'Raw_Hosp_Name', 'Brand', 'Form', 'Specifications',\n",
       "       'Pack_Number', 'Manufacturer', 'Molecule', 'Source', 'Corp', 'Route',\n",
       "       'ORG_Measure', 'Sales', 'Units', 'Units_Box', 'Path', 'Sheet', 'min1'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+-------+-------+------------------+-------------------+----------+\n",
      "|    ID|Molecule|201701|201702|201703|201704|201705|201706|201707|201708|201709|201710|201711|201712|201801|201802|201803|201804|201805|201806|201807|201808|201809|201810|201811|201812|201901|201902|201903|201904|201905|201906|201907|201908|201909|201910|201911|201912|202001|202002|202003|202004|202005|202006|202007|202008|202009|202010|202011|row_max|row_min|          mean_adj|           min_diff|       PHA|\n",
      "+------+--------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+-------+-------+------------------+-------------------+----------+\n",
      "|200251|埃克替尼|     3|     2|     2|     2|     2|     1|     1|     1|     1|     2|     1|     1|     1|     2|     1|     2|     1|     1|     1|     1|     1|     2|     2|     1|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|      3|      1|1.4090909090909092|0.40909090909090917|PHA0013283|\n",
      "+------+--------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+-------+-------+------------------+-------------------+----------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a=time.time()\n",
    "schema = StructType([\n",
    "        StructField(\"ID\", StringType(), True),\n",
    "        StructField(\"Molecule\", StringType(), True),\n",
    "        StructField(\"level\", IntegerType(), True),\n",
    "        StructField(\"month\", StringType(), True)\n",
    "        ])\n",
    "@pandas_udf(schema, PandasUDFType.GROUPED_MAP)\n",
    "def pudf_cumsum_rank(pdf):\n",
    "    return func_cumsum_rank(pdf, grouplist=['ID', \"Molecule\"], sumcol='Sales')\n",
    "\n",
    "out = Raw_data.groupby([\"Date\"]).apply(pudf_cumsum_rank)\n",
    "out2 = out.groupby('ID', \"Molecule\").pivot('month').agg(func.sum('level')).persist()\n",
    "out2 = cumsum_rank_2(out2, grouplist=['ID', 'Molecule'])\n",
    "out2.show(1)\n",
    "b=time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-12.21456003189087"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a-b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deal_ID_length(df):\n",
    "    # ID不足7位的补足0到6位\n",
    "    # 国药诚信医院编码长度是7位数字，cpa医院编码是6位数字。\n",
    "    df = df.withColumn(\"ID\", df[\"ID\"].cast(StringType()))\n",
    "    # 去掉末尾的.0\n",
    "    df = df.withColumn(\"ID\", func.regexp_replace(\"ID\", \"\\\\.0\", \"\"))\n",
    "    df = df.withColumn(\"ID\", func.when(func.length(df.ID) < 7, func.lpad(df.ID, 6, \"0\")).otherwise(df.ID))\n",
    "    return df\n",
    "\n",
    "@udf(DoubleType())\n",
    "def mean_adj(*cols):\n",
    "    # 以行为单位，去掉一个最大值和一个最小值求平均\n",
    "    import numpy as np\n",
    "    row_max = cols[0]\n",
    "    row_min = cols[1]\n",
    "    others = cols[2:]\n",
    "    row_mean_list = [x for x in others if x is not None]\n",
    "    if len(row_mean_list) > 3:\n",
    "        row_mean = (np.sum(row_mean_list) - row_max - row_min)/(len(row_mean_list) -2)\n",
    "    else:\n",
    "        row_mean = 0\n",
    "    return float(row_mean)\n",
    "\n",
    "\n",
    "@udf(DoubleType())\n",
    "def min_diff(row_max, row_min, mean_adj):\n",
    "    # row_min 与 mean_adj 的差值\n",
    "    import numpy as np\n",
    "    if mean_adj is not None:\n",
    "        # diff1 = abs(row_max - mean_adj)\n",
    "        diff2 = abs(row_min - mean_adj)\n",
    "        row_diff = diff2\n",
    "    else:\n",
    "        row_diff = 0\n",
    "    return float(row_diff)\n",
    "\n",
    "def cumsum_rank_2(check_num, grouplist):\n",
    "    check_num_cols = check_num.columns\n",
    "    check_num_cols = list(set(check_num_cols) - set(grouplist))\n",
    "    for each in check_num_cols:\n",
    "        check_num = check_num.withColumn(each, check_num[each].cast(IntegerType()))\n",
    "\n",
    "    # 平均值计算\n",
    "    check_num = check_num.withColumn(\"row_max\", func.greatest(*check_num_cols)) \\\n",
    "                    .withColumn(\"row_min\", func.least(*check_num_cols))\n",
    "    check_num = check_num.withColumn(\"mean_adj\", \n",
    "                        mean_adj(func.col('row_max'), func.col('row_min'), *(func.col(x) for x in check_num_cols)))\n",
    "    check_num = check_num.withColumn(\"mean_adj\", func.when(func.col('mean_adj') == 0, func.lit(None)).otherwise(func.col('mean_adj')))\n",
    "    # row_min 与 mean_adj 的差值\n",
    "    check_num = check_num.withColumn(\"min_diff\", min_diff(func.col('row_max'), func.col('row_min'), func.col('mean_adj')))\n",
    "    check_num = check_num.withColumn(\"min_diff\", func.when(func.col('mean_adj').isNull(), func.lit(None)).otherwise(func.col('min_diff')))\n",
    "\n",
    "    # 匹配PHA    \n",
    "    cpa_pha_mapping = spark.read.parquet(cpa_pha_mapping_path)\n",
    "    cpa_pha_mapping = cpa_pha_mapping.where(cpa_pha_mapping[\"推荐版本\"] == 1).select('ID', 'PHA').distinct()\n",
    "    cpa_pha_mapping = deal_ID_length(cpa_pha_mapping)\n",
    "\n",
    "    check_num = deal_ID_length(check_num)\n",
    "    check_num = check_num.join(cpa_pha_mapping, on='ID', how='left')\n",
    "    \n",
    "    return check_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[project: string, project_score: string, Date: string, ATC: string, 标准通用名: string, 标准商品名: string, 标准剂型: string, 标准规格: string, 标准包装数量: string, 标准生产企业: string, 标准省份名称: string, 标准城市名称: string, PACK_ID: string, Sales: string, Units: string, PANEL: string]"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.csv('s3a://ph-stream/common/public/max_result/0.0.5/extract_data_out/out_2021_01_21_test/out_2021_01_21_test.csv/', header=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+------+-----+----------+------------------+--------+--------+------------+------------------------+------------+------------------+-------+--------+------+-----+\n",
      "|project|project_score|  Date|  ATC|标准通用名|        标准商品名|标准剂型|标准规格|标准包装数量|            标准生产企业|标准省份名称|      标准城市名称|PACK_ID|   Sales| Units|PANEL|\n",
      "+-------+-------------+------+-----+----------+------------------+--------+--------+------------+------------------------+------------+------------------+-------+--------+------+-----+\n",
      "|Servier|            5|201806|A10J1|  二甲双胍|盐酸二甲双胍肠溶片|  肠溶片|   250MG|          48|北京利龄恒泰药业有限公司|      山西省|            忻州市|1244202| 2676.88|  7006|  0.0|\n",
      "|Servier|            5|201902|A10J1|  二甲双胍|            格华止|  缓释片|   500MG|          30|                默克集团|      海南省|省直辖县级行政单位|6489506|  529.19|   274|  0.0|\n",
      "|Servier|            5|201801|A10H0|  格列喹酮|            糖适平|    片剂|    30MG|          30|                双鹤集团|      河北省|            邯郸市|  90004|14436.71|  6659|  0.0|\n",
      "|Servier|            5|201902|A10J1|  二甲双胍|            麦特美|  缓释片|   500MG|          30|青岛黄海制药有限责任公司|      河北省|            保定市|1912802|21256.19| 49433|  0.0|\n",
      "|Servier|            5|201807|A10J1|  二甲双胍|          麦克罗辛|  缓释片|   500MG|          24|        天方药业有限公司|      安徽省|            蚌埠市|1845102|47901.62|125233|  0.0|\n",
      "|Servier|            5|201810|A10J1|  二甲双胍|            格华止|薄膜衣片|   850MG|          20|                默克集团|      湖北省|            武汉市|6489502|79809.21| 49334|  0.0|\n",
      "|Servier|            5|201801|A10J1|  二甲双胍|            安多可|  肠溶片|   850MG|          60|贵州天安药业股份有限公司|      海南省|省直辖县级行政单位| 706712|  252.18|   145|  0.0|\n",
      "|Servier|            5|201806|A10J1|  二甲双胍|            君力达|肠溶胶囊|   250MG|          24|    北京圣永制药有限公司|      安徽省|            铜陵市|1519802| 1303.37|  2941|  0.0|\n",
      "|Servier|            5|201803|A10J1|  二甲双胍|盐酸二甲双胍缓释片|  缓释片|   500MG|          30|江苏德源药业股份有限公司|      辽宁省|          葫芦岛市|4343908|  1824.0|  6000|  1.0|\n",
      "|Servier|            5|201907|A10J1|  二甲双胍|            麦特美|  缓释片|   500MG|          30|青岛黄海制药有限责任公司|      山东省|            青岛市|1912802|245358.0|570600|  1.0|\n",
      "|Servier|            5|201808|A10H0|  格列喹酮|            糖适平|    片剂|    30MG|          30|                双鹤集团|      甘肃省|            定西市|  90004|  7874.4|  7200|  1.0|\n",
      "|Servier|            5|201904|A10J1|  二甲双胍|    盐酸二甲双胍片|薄膜衣片|   250MG|          48|    北京中惠药业有限公司|      甘肃省|            平凉市|3211902|   750.0| 14400|  1.0|\n",
      "|Servier|            5|201809|A10J1|  二甲双胍|              泰白|  缓释片|   500MG|          30|            正大天晴集团|      湖南省|          张家界市|1693206|   46.69|    87|  0.0|\n",
      "|Servier|            5|201809|A10J1|  二甲双胍|            安多可|  肠溶片|   500MG|          60|贵州天安药业股份有限公司|      青海省|    海北藏族自治州| 706716|  911.54|  3429|  0.0|\n",
      "|Servier|            5|201809|A10J1|  二甲双胍|              卜可|  缓释片|   500MG|          30|                双鹤集团|      河北省|          石家庄市|3279404| 15851.5| 31703|  0.0|\n",
      "|Servier|            5|201807|A10J1|  二甲双胍|            格华止|薄膜衣片|   500MG|          20|                默克集团|    黑龙江省|            绥化市|6489504|  5782.5|  5000|  1.0|\n",
      "|Servier|            5|201807|A10J1|  二甲双胍|            都乐宁|  缓释片|   500MG|          30|  重庆康刻尔制药有限公司|      安徽省|            宿州市|4423902| 6991.49|  9072|  0.0|\n",
      "|Servier|            5|201806|A10H0|  格列喹酮|            糖适平|    片剂|    30MG|          60|                双鹤集团|      山东省|            青岛市|  90002| 45528.0| 42000|  1.0|\n",
      "|Servier|            5|201803|A10J1|  二甲双胍|            君力达|肠溶胶囊|   500MG|          48|    北京圣永制药有限公司|内蒙古自治区|        巴彦淖尔市|1519810| 4357.84|  7062|  0.0|\n",
      "|Servier|            5|201901|A10J1|  二甲双胍|    盐酸二甲双胍片|    片剂|   250MG|          48|上海上药信谊药厂有限公司|内蒙古自治区|        鄂尔多斯市| 819204|  2700.0| 14400|  1.0|\n",
      "+-------+-------------+------+-----+----------+------------------+--------+--------+------------+------------------------+------------+------------------+-------+--------+------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
